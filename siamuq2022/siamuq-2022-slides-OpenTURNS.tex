% Copyright (C) 2022 - Joseph Mur√©

\documentclass{beamer}

%\setbeameroption{hide notes}
%\setbeameroption{show notes}
%\setbeameroption{show only notes}

\include{macros}

\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}

\usepackage{tikz}
\usetikzlibrary{positioning}

\renewcommand{\footnotesize}{\tiny}

\addbibresource{siamuq-2022-slides-OpenTURNS.bib}

\title[OpenTURNS]{Overview of OpenTURNS, its new features and its graphical user interface}

\author[Mur\'e et al.]{
J. Mur\'e \inst{1} \and
M. Baudin \inst{1} \and
J. Pelamatti \inst{1} \and
A. Dutfoy \inst{1} \and
O.~Mircescu \inst{1} \and
J. Schueller \inst{2} \and
T. Yalamas \inst{2}
}

\institute[EDF-Phim\'eca]{
\inst{1} EDF R\&D. 6, quai Watier, 78401, Chatou Cedex - France, joseph.mure@edf.fr \and %
\inst{2} Phimeca Engineering. 18/20 boulevard de Reuilly, 75012 Paris - France, yalamas@phimeca.com
}

\date[]{April 15th 2022, SIAM UQ 2022, Atlanta, US}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},   commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\tiny,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=1,
  numbers=none
}

\lstset{style=mystyle, language=python}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \begin{frame}
  \titlepage
  
  \begin{columns}
    \column{0.45\textwidth}
  \begin{center}
\includegraphics[height=0.15\textheight]{figures/edf.jpg}
\end{center}
    \column{0.1\textwidth}
	
    \column{0.45\textwidth}
  \begin{center}
\includegraphics[height=0.15\textheight]{figures/logo_phimeca.png}
\end{center}
  \end{columns}

  \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{frame}
% \frametitle{Contents}
% \tableofcontents
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{OpenTURNS: \url{www.openturns.org}}


    \begin{center}
    \includegraphics[width=0.8\textwidth]{figures/OT.png}
    \end{center}
	
\begin{itemize}
\item Multivariate probabilistic modeling including dependence
\item Numerical tools dedicated to the treatment of uncertainties
\item Generic coupling to any type of physical model
\item Open source, LGPL licensed, C++/Python library
\end{itemize}


\end{frame}

\note{
OpenTURNS is software for uncertainty quantification, uncertainty propagation,
sensitivity analysis and metamodeling. 

It is available with the open source LGPL licence on Linux, Windows and macOS.

In order to use OpenTURNS, you can use directly the C++ library, or 
program your Python scripts. 
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{OpenTURNS: \url{www.openturns.org}}

\begin{center}
   \begin{tabular}{ccccc}
   \includegraphics[width=0.07\textwidth]{figures/logoEDF_Anne.png}&
   \includegraphics[width=0.12\textwidth]{figures/LogoAirbus.png}&
   \includegraphics[width=0.12\textwidth]{figures/logo_phimeca.png}&
   \includegraphics[width=0.12\textwidth]{figures/logo_Imacs.png}
   \includegraphics[width=0.30\textwidth]{figures/logo_ONERA.jpg}&
   \end{tabular}
\end{center}

\vspace*{0.05cm}
\begin{itemize}
\item Linux, Windows, macOS
\item First release : 2007
\item 5 full time developers
\item Users $\approx 1000$, mainly in France
(785 000 Total Conda downloads)
\item Project size : 800  classes, more than 6000 services
\end{itemize}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[containsverbatim]
  \frametitle{OpenTURNS: content}
  
  \begin{scriptsize}
  
  \begin{minipage}[t]{0.33\textwidth}
  \begin{itemize}
  \item Data analysis
  \begin{itemize}
  \tiny
  \item Distribution fitting
  \item Statistical tests
  \item Estimate dependency and copulas
  \item Estimate stochastic processes
  \end{itemize}
  \end{itemize}
  \end{minipage}%
  \begin{minipage}[t]{0.33\textwidth}
  \begin{itemize}
  \item Probabilistic modeling
  \begin{itemize}
  \tiny
  \item Dependence modeling
  \item Univariate distributions
  \item Multivariate distrbutions
  \item Copulas
  \item Processes 
  \item Covariance kernels
  \end{itemize}
  \end{itemize}
  \end{minipage}%
  \begin{minipage}[t]{0.33\textwidth}
  \begin{itemize}
  \item Surrogate models
  \begin{itemize}
  \tiny
  \item Linear regression
  \item Polynomial chaos expansion
  \item Gaussian  process regression
  \item Spectral methods
  \item Low rank tensors
  \item Fields metamodel
  \end{itemize}
  \end{itemize}
  \end{minipage}
  
  \vspace{20pt}  
  
  \begin{minipage}[t]{0.33\textwidth}
  \begin{itemize}
  \item Reliability, sensitivity
  \begin{itemize}
  \tiny
  \item Sampling methods
  \item Approximation methods
  \item Sensitivity analysis
  \item Design of experiments
  \end{itemize}
  \end{itemize}
  \end{minipage}%
  \begin{minipage}[t]{0.33\textwidth}
  \begin{itemize}
  \item Calibration
  \begin{itemize}
  \tiny
  \item Least squares calibration
  \item Gaussian calibration
  \item Bayesian calibration
  \end{itemize}
  \end{itemize}
  \end{minipage}%
  \begin{minipage}[t]{0.33\textwidth}
  \begin{itemize}
  \item Numerical methods
  \begin{itemize}
  \tiny
  \item Optimization
  \item Integration
  \item Least squares 
  \item Meshing
  \item Coupling with external codes
  \end{itemize}
  \end{itemize}
  \end{minipage}
  \end{scriptsize}

  \begin{tabular}{@{}c@{}c@{}c@{}c@{}c@{}}
  \includegraphics[width=0.2\textwidth]{figures/plot_kriging.png}&
  \includegraphics[width=0.2\textwidth]{figures/plot_random_walk.png}&
  \includegraphics[width=0.2\textwidth]{figures/plot_sobol_field.png}&
  \includegraphics[width=0.2\textwidth]{figures/plot_monte_carlo.png}&
  \includegraphics[width=0.2\textwidth]{figures/plot_distribution_fitting.png}
  \end{tabular}
\end{frame}
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[containsverbatim]
  \frametitle{OpenTURNS: documentation}
  
  \small{
  
  \begin{columns}
      \column{0.5\textwidth}
  
      \begin{center}
      \includegraphics[width=0.8\textwidth]{figures/exClasses.png}
      \end{center}
  
      \column{0.5\textwidth}
  
    \begin{itemize}
    \item \underline{Content}: 
    \begin{itemize}
    \item Programming interface (API)
    \item Examples
    \item Theory
    \end{itemize}
    \item \emph{All} classes and methods 
    are documented, partly automatically.
    \item Examples are automatically tested at \emph{each} update 
    of the code and outputs are checked.
    \end{itemize}
    
  \end{columns}
  
  }
\end{frame}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[containsverbatim]
  \frametitle{OpenTURNS: practical use}
  
  \small
  \begin{itemize}
  \item Compatibility with most popular python packages
  \begin{itemize}
  \scriptsize
  \item Numpy
  \item Scipy
  \item Matplotlib
  \item scikit-learn
  \end{itemize}
  \vspace{10pt}
  \item Parallel computational with shared memory (TBB)
  \vspace{10pt}
  \item Optimized linear algebra with LAPACK and BLAS 
  \vspace{10pt}
  \item Possibility to interface with a computation cluster
  \vspace{10pt}
  \item Focused towards handling numerical data
  \vspace{10pt}
  \item Installation through conda, pip, packages for various Linux distros and source code
  \end{itemize}
\end{frame}
  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[containsverbatim]
  \frametitle{Coupling OpenTURNS with computer codes}
  
  \small
  
  OpenTURNS provides a text file exchange based interface in order to perform analyses on complex computer codes
  
  \vspace{10pt}
  
  \begin{columns}
      \column{0.6\textwidth}
      
  \centering
  
  \includegraphics[width=1.\textwidth]{figures/Coupling.png}
  
      \column{0.4\textwidth}
  
  \begin{itemize}
  \item Replaces the need for input/output text parsers
  \item Wraps a simulation code under the form of a standard python function
  \item Allows to interface OpenTURNS with a cluster
  \item \href{https://openturns.github.io/otwrapy/master/index.html}{otwrapy}: interfacing tool to allow easy parallelization
  \end{itemize}
  
  \end{columns}
  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{OpenTURNS 1.19: New capabilities}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
  \frametitle{OpenTURNS Sensitivity analysis : HSIC indices \footfullcite{gretton2005} \footfullcite{daveiga2015}}
  
  \small
      \begin{itemize}
        \item Probabilistic modeling of $d$ input physical variables and black-box model:
        \begin{equation*}
        \mathbf{X} = (X_1, X_2, \dots, X_d)^{\top} \sim P_{\mathbf{X}} \quad\mbox{over}\quad \mathcal{X} = \times_{i=1}^{d} \mathcal{X}_i
        \end{equation*}
        \vspace{-10pt}
        \begin{equation*}
        \mathcal{M} : \left|\begin{array}{rcl}
        \mathcal{X} \subseteq \mathbb{R}^d & \longrightarrow & \mathcal{Y} \subseteq \mathbb{R}\\
        \mathbf{X} & \longrightarrow & Y = \mathcal{M}(\mathbf{X})
        \end{array}
        \right.
        \end{equation*}
        \vspace{10pt}
        \item \textbf{Learning sample} $\rightarrow$ a $n$-size i.i.d. sample of the couple $(\mathbf{X},Y)$:
        \begin{equation*}
        \left(\mathbf{X}^{(j)}, Y^{(j)}\right)_{(1\leq j\leq n)} = \left(X_1^{(j)}, X_2^{(j)}, \dots, X_d^{(j)}, Y^{(j)}\right)_{(1\leq j\leq n)}
        \end{equation*}
        with $P_{\mathbf{X}^{(j)}} = P_{\mathbf{X}}$ and $Y^{(j)} = \mathcal{M}\left(X_1^{(j)}, X_2^{(j)}, \dots, X_d^{(j)}\right)$, $\forall j \in \{1,\dots,n\}$
      \end{itemize}
  
  \end{frame}

  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
  \begin{frame}
  \frametitle{OpenTURNS Sensitivity analysis : HSIC indices}
  
  \small
  
    \begin{itemize}
    \item Let $\mathcal{V}_i$ be a Reproducing Kernel Hibert Space over $\mathcal{X}_i \times \mathcal{Y} $ with kernel $v_i(\cdot, \cdot)$.
    \item We consider the mean embedding of $P_{X_i,Y}$ and $P_{X_i} P_Y$ in $\mathcal{V}$:
    \begin{itemize}
        \item $\mu [ P_{X_i, Y} ] = \iint_{\mathcal{X}_i \times \mathcal{Y}} v_i((x,y), \cdot) dP_{X_i, Y}(x,y)$
        \item $\mu [ P_{X_i} P_Y ] = \int_{\mathcal{Y}} \left[ \int_{\mathcal{X}_i} v_i((x,y), \cdot) dP_{X_i}(x) \right] dP_Y(y)$
    \end{itemize}
    \item We now have a dependence measure under the form of:
    \begin{equation*}
        \Delta := || \mu [ P_{Y,X_i} ] - \mu [ P_YP_{X_i} ]  || \longleftrightarrow HSIC(Y,X_i) = \Delta^2
    \end{equation*}
    \end{itemize}
  
  \begin{figure}
      \includegraphics[width=0.85\textwidth]{figures/RKHSEmbedding.png}
  \end{figure}
  
  \end{frame}
  
  
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
  \begin{frame}
  \frametitle{OpenTURNS Sensitivity analysis : HSIC indices}
  
  \small
  \begin{block}{A few advantages}
  
  \begin{itemize}
    \item Data efficient
    \item Given data estimators
    \item Computational cost scales linearly with the problem dimension
    \item Associated statistical test
    \item Can be used when dealing with non continuous variables
    \begin{itemize}
    \item discrete/categorical variables
    \item graphs
    \item stochastic codes
    \end{itemize}
    \vspace{5pt}
    \item Formulation holds in case of dependence between inputs
    \begin{itemize}
    \item \textcolor{red}{Interpretation of results becomes complicated!}
    \end{itemize}
  \end{itemize}
  
  \end{block}
  
  \end{frame}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{frame}
\frametitle{OpenTURNS Sensitivity analysis : HSIC indices}

\small

\begin{block}{Statistical hypothesis testing with HSIC}
\begin{itemize}
\item We consider the following statistical test $\mathcal{T}$
\small
\begin{equation*}
\mathcal{T}: \textrm{Test}\quad "(\mathcal{H}_{0,i}):\mathrm{HSIC}(X_i,Y) = 0"\quad\textrm{vs.}\quad "(\mathcal{H}_{1,i}):\mathrm{HSIC}(X_i,Y) > 0"
\end{equation*}

\underline{\textcolor{red}{$\mathcal{H}_{0,i}$ : $X_i$ and $Y$ are independent}}

\normalsize
\item  $\widehat{S}_{\mathcal{T}} := n \times \widehat{\mathrm{HSIC}}(X_i,Y)$ is the test statistic, $\widehat{S}_{\mathcal{T},\textrm{obs}}$ its observed value.

\item \textbf{p-value} associated to the test $\mathcal{T}$:
\begin{equation*}
p_{\textrm{val}} = \mathbb{P}\left(\widehat{S}_{\mathcal{T}} \geq \widehat{S}_{\mathcal{T},\textrm{obs}}~|~\mathcal{H}_{0,i}\right)
\end{equation*}
\vspace{-10pt}
\begin{itemize}
\item Asymptotic estimator (large data set)
\item Permutation-based estimator (small data set)
\end{itemize}
\end{itemize}
\end{block}

\end{frame}
  
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{frame}[containsverbatim]
\frametitle{OpenTURNS Sensitivity analysis : HSIC indices}

\begin{lstlisting}
globHSIC = ot.HSICEstimatorGlobalSensitivity(X, Y, kernel_list, ot.HSICUStat())
globHSIC.drawPValuesAsymptotic()
globHSIC.drawPValuesPermutation()
\end{lstlisting}


\begin{columns}
  \column{0.49\textwidth}
  \includegraphics[width=.99\textwidth]{figures/HSIC_Pvalues_asymptotic.png}
  \column{0.49\textwidth}
  \includegraphics[width=.99\textwidth]{figures/HSIC_Pvalues_permutation.png}
\end{columns}

\medskip

\small 
HSIC indices are compatible with various sensitivity analysis objectives:

\begin{itemize}
  \item Global analysis
  \item Target analysis, e.g. sensitivity of the event $\{Y>s\}$ to the $X_i$ ($s \in \mathbb{R}$)
  \item Conditional analysis, e.g. sensitivity of $Y$ to the $X_i$ conditionally to $Y>s$
\end{itemize}
  
\end{frame}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[containsverbatim]
\frametitle{OpenTURNS: Metropolis-Hastings}
\small
We want to sample from the joint distribution $\pi$ of 3 random variables $X, Y, Z$.
Here is one step of the algorithm, starting from the point $(x, y, z)$:

\begin{enumerate}
\item Simulate a candidate $(x',y',z') \sim q( \cdot | x, y, z)$ for some conditional distribution $q$.
\item Compute
$
\alpha(x',y',z' | x,y,z) = \min \left\{ \frac{\pi(x',y',z') \, q(x,y,z | x',y',z')}{\pi(x,y,z) q(x',y',z' | x,y,z)} , 1 \right\}.
$
\item Simulate $u \sim \mathcal{U}(0,1)$. If $u \leqslant \alpha(x',y',z' | x,y,z)$,
then the next state is $(x',y',z')$, otherwise it is $(x,y,z)$.
\end{enumerate}

\begin{block}{Random walk Metropolis-Hastings}

When $q(\cdot | x, y, z) = (x,y,z) + \mu$, where $\mu$ is a distribution that does not depend on $(x,y,z)$,
the  algorithm is called ``Random walk Metropolis-Hastings'' and $\mu$ is called the ``proposal distribution''.

\begin{lstlisting}
init = [x_start, y_start, z_start]
proposal = ot.Normal(3) # steps follow a 3d normal distribution
mh = ot.RandomWalkMetropolisHastings(logdensity, support, init, proposal)
sample = mh.getSample(1000)
\end{lstlisting}
\end{block}

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{OpenTURNS: Metropolis-Hastings variants}
\centering

\begin{block}{Single component randomwalk Metropolis-Hastings}
\medskip
\centering
\begin{tikzpicture}[
squarednode/.style={rectangle, draw=green!60, fill=green!5, very thick, minimum size=5mm},
]
%Nodes
\node[squarednode]      (mhx)                              {\texttt{mh\_x}};
\node[squarednode]      (mhy)       [right=of mhx] {{\texttt{mh\_y}}};
\node[squarednode]      (mhz)       [right=of mhy] {{\texttt{mh\_z}}};

%Lines
\draw[->] (mhx.east) -- (mhy.west);
\draw[->] (mhy.east) -- (mhz.west);
\draw[->] (mhz.east) to [out=340,in=200,looseness=1] (mhx.west);
\end{tikzpicture}

\begin{lstlisting}
prop_dim1 = ot.Normal(1) # steps follow a 1d normal distribution along one axis
mh_x = ot.RandomWalkMetropolisHastings(logdensity, support, init, prop_dim1, [0])
mh_y = ot.RandomWalkMetropolisHastings(logdensity, support, init, prop_dim1, [1])
mh_z = ot.RandomWalkMetropolisHastings(logdensity, support, init, prop_dim1, [2])
sample = ot.Gibbs([mh_x, mh_y, mh_z]).getSample(1000)
\end{lstlisting}

\end{block}

\begin{block}{Blocks of components can be considered}
\medskip
\centering
\begin{tikzpicture}[
squarednode/.style={rectangle, draw=green!60, fill=green!5, very thick, minimum size=5mm},
]
%Nodes
\node[squarednode]      (mhxy)                     {\texttt{mh\_xy}};
\node[squarednode]      (mhz)       [right=of mhxy] {\texttt{mh\_z}};

%Lines
\draw[->] (mhxy.east) -- (mhz.west);
\draw[->] (mhz.east) to [out=340,in=200,looseness=1.5] (mhxy.west);

\end{tikzpicture}

\begin{lstlisting}
prop_xy = ot.Normal(2) # steps follows a 2d normal distribution for (X,Y)
prop_z = ot.Normal(1)  # steps follow  a 1d normal distribution for Z
mh_xy = ot.RandomWalkMetropolisHastings(logdensity, support, init, prop_xy, [0, 1])
mh_z = ot.RandomWalkMetropolisHastings(logdensity, support, init, proposal, [2])
sample = ot.Gibbs([mh_xy, mh_z]).getSample(1000)
\end{lstlisting}  

\end{block}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[containsverbatim]
\frametitle{OpenTURNS: conditional samplers in Gibbs algorithms}
\small
Assume we have a direct sampler for $Z$ conditionally on $X$ and $Y$:
this sampler should be passed to the \texttt{RandomVectorMetropolisHastings} class.

\begin{lstlisting}
  z_sampler = ot.RandomVector(...) # build a direct sampler for Z | X, Y
  # Typically, Z depends on a function of (X,Y)
  # Write it as a function of (X,Y,Z) for the sake of genericity:
  f = ot.Function(...) # function f(X,Y,Z) which outputs the parameters of z_sampler
  mh_z = ot.RandomVectorMetropolisHastings(z_sampler, init, [2], f)

  prop_xy = ot.Normal(2) # steps follows a 2d normal distribution for (X,Y)
  mh_xy = ot.RandomWalkMetropolisHastings(logdensity, support, init, prop_xy, [0, 1])

  sample = ot.Gibbs([mh_x, mh_y, mh_z]).getSample(1000)
\end{lstlisting}
\centering
\includegraphics[width=.33\textwidth]{figures/gibbs_1.png}
\includegraphics[width=.66\textwidth]{figures/gibbs_2.png}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[containsverbatim]
  \frametitle{OpenTURNS: iterative statistics \footfullcite{pebay2008}  \footfullcite{meng2015}}
\small
\begin{columns}

  \column{0.54\textwidth}


  \begin{block}{Iterative moment estimation}
  \begin{lstlisting}
  iterMoments = ot.IterativeMoments(order, dim)

  # Iteratively compute the mean
  size = 2000
  meanEvolution = ot.Sample()
  for i in range(size):
      point = distNormal.getRealization()
      iterMoments.increment(point)
      meanEvolution.add(iterMoments.getMean())

  # Display the evolution
  ran = range(1, size + 1)
  iter_sample = ot.Sample.BuildFromPoint(ran)
  curve = ot.Curve(iter_sample, meanEvolution)
  graph = ot.Graph("Evolution of the mean",
                    "iteration nb",
                    "mean",
                    True)
  graph.add(curve)
  graph.setLogScale(ot.GraphImplementation.LOGX)
  view = otv.View(graph)
  \end{lstlisting}
  \end{block}

  \column{0.44\textwidth}

  \includegraphics[width=\textwidth]{figures/iter_mean.png}

  \begin{block}{Other available iterative stats}
    \begin{itemize}
      \item Extrema
      \item Threshold exceedance probability
    \end{itemize}
  \end{block}

\end{columns}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{PERSALYS, the graphical user interface}

\begin{frame}
\frametitle{PERSALYS, the graphical user interface of \ot{}}
	
\begin{columns}
\column{0.7\textwidth}
	
\begin{itemize}
\item Provide a graphical interface of 
\ot{} in and out of the SALOME integration platform
\item Features : probabilistic model, 
	distribution fitting, central tendency, 
  sensitivity analysis, probability estimate, 
	surrogate modeling (polynomial chaos, kriging), screening (Morris), 
	optimization, design of experiments
\item GUI language : English, French

\item Partners : EDF, Phim\'eca
\item Licence : LGPL

\item Schedule : Since summer 2016, two releases per year, currently V12
\item On the internet (free) : SALOME\_EDF since 2018 on 
\url{www.salome-platform.org}

\end{itemize}

\column{0.3\textwidth}

\begin{center}
\includegraphics[width=0.95\textwidth]{figures/PERSALYS-LOGO.png}
\end{center}

\end{columns}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Calibration}

Given a physical model $h$, observed inputs $\boldsymbol{x}$, 
observed outputs $y$ we can calibrate $\boldsymbol{\theta}$ so that 
$$
y = h(\boldsymbol{x}, \boldsymbol{\theta}) + \epsilon
$$
where $\epsilon$ is a random (Gaussian) variable.

Calibration outputs:
\begin{itemize}
\item the optimal value estimator $\hat{\boldsymbol{\theta}}$,
\item the distribution of $\hat{\boldsymbol{\theta}}$ and a confidence or credibility 
interval of each marginal,
\item the distribution of the residuals $\epsilon = y - h(\boldsymbol{x}, \hat{\boldsymbol{\theta}})$.
\end{itemize}

We implemented 4 methods\footnote{M. Baudin \& R. Lebrun, Linear algebra of linear and nonlinear Bayesian calibration. UNCECOMP 2021.} :

\begin{center}
\begin{tabular}{lll}
                     & {\bf Linear} & {\bf Non Linear} \\
\hline
No prior             & Least squares & Least squares \\
                     & linear        &  non linear \\
\hline
Gaussian prior       & Linear calib. & 3DVAR \\
\hline
\end{tabular}
\end{center}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Calibration}

In PERSALYS, this requires:
\begin{itemize}
\item a physical model $h$ with parameters to calibrate, 
\item a data file containing the observed inputs and outputs.
\end{itemize}

\begin{center}
\includegraphics[width=0.35\textwidth]{figures/calibration_observations_contextMenu.png}
\end{center}

\begin{columns}
  \column{0.5\textwidth}

On output, we present the optimal value of each calibrated parameter, 
along with a 95\% confidence interval.

\column{0.5\textwidth}

\begin{center}
\includegraphics[width=0.9\textwidth]{figures/calibration-ks-zv-zm-optimal_focus.png}
\end{center}

\end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
  \frametitle{Calibration}
    
  \begin{center}
  \includegraphics[width=0.86\textwidth]{figures/calibration-Zv-prior-posterior.png}
  \end{center}
  
  \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{What's next ?}
  \begin{columns}
    \column{0.5\textwidth}

PERSALYS Roadmap : 
\begin{itemize}
\item New surrogate models
\item 2D Fields, 3D Fields
\item In-Situ fields based on the MELISSA library (with INRIA): 
when we cannot store the whole sample in memory or on the hard drive, 
update the statistics (e.g. the mean, Sobol' indices) sequentially, 
with distributed computing. 
\end{itemize}

    \column{0.5\textwidth}

\begin{center}
\includegraphics[height=0.5\textheight]{figures/image034.png}
\end{center}

	\end{columns}

\end{frame}

\note{

The next features we plan to add to PERSALYS is the management
of 2D and 3D stochastic fields. 

We also work on the use of the MELISSA software which performs 
in-situ studies. 

This library allows to perform UQ studies in situations 
where we cannot store more than a couple of multidimensional fields 
in memory or on the hard drive. 
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{The end}

\begin{center}
Thanks !
\end{center}

\begin{center}
Questions ?
\end{center}

\begin{columns}
\column{0.5\textwidth}
\centering
\includegraphics[width=0.5\textwidth]{figures/logo-openturns.png}
\column{0.5\textwidth}

\includegraphics[width=0.8\textwidth]{figures/PERSALYS-LOGO.png}

\end{columns}

\end{frame}
  
\end{document}
